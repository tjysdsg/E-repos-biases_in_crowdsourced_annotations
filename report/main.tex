% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{acl}
\usepackage{acl}

\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{threeparttable}

\title{11830 Report: Biases in Crowdsourced Annotations}

\author{
    Jiyang Tang \\
    \texttt{jiyangta@andrew.cmu.edu}
}

\begin{document}
    \maketitle
    \begin{abstract}
        This document is a supplement to the general instructions for *ACL authors. It contains instructions for using the \LaTeX{} style files for ACL conferences.
        The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like.
        These instructions should be used both for papers submitted for review and for final versions of accepted papers.
    \end{abstract}


    \section{Introduction}

    The Stanford Natural Language Inference (SNLI) corpus\footnote{\url{https://nlp.stanford.edu/projects/snli/}} is a large crowdsourced natural language
    inference dataset~\cite{snli}.
    For each premise sentence, annotators were asked to write a hypothesis that is either a contradiction,
    neutral statement, or entailment of the premise.

    The data was collected from Amazon Mechanical Turk (MTurk) crowdsourcing service.
    Previous studies have shown that MTurk crowd-workers tend to have lower income, higher education levels, and
    lower average ages~\cite{mturks_demography}.
    We believe that biases are inevitably propagated from the dataset authors to the premise text, and from the
    crowd-workers to the hypothesis text.
    Therefore, we will analyze the biases in the data by performing word association tests in this report.


    \section{Method}

    \subsection{Pointwise Mutual Information}

    Pointwise Mutual Information (PMI) is used to measure how much word $w_i$ is associated with word
    $w_j$~\cite{pmi,speech_and_nlp_book}.
    PMI is calculated as follows:

    \[
        \text{PMI}(w_i, w_j) = log_2\frac{N\cdot c(w_i, w_j)}{c(w_i)c(w_j)}
    \]
    where $N$ is the total number of sentences in the corpus,
    $c(w_i,w_j)$ is number of times $w_i$ and $w_j$ co-occur in a sentence,
    $c(w_i)$ is the number of times $w_i$ occurs in the corpus,
    and $c(w_j)$ is the number of times $w_j$ occurs in the corpus.

    Note that if a pair of words $w_i$ and $w_j$ occurs multiple times in the same sentence, $c(w_i,w_j)$ is counted
    as $1$.

    PMI ranges from negative infinity to positive infinity.
    Large PMI suggests high word association.
    Negative PMI implies two words co-occur less often than by chance and is unreliable in
    practice~\cite{speech_and_nlp_book}.


    \section{Experiments}

    \subsection{Data Preprocessing}

    As mentioned before, each data sample contains a premise and a hypothesis.
    Note that multiple hypotheses might be generated from the same premise, therefore duplicated premise text is
    removed.
    All words are converted into its lower case form and stop words are removed.
    Then we use \texttt{spaCy}~\cite{spacy} \texttt{en\_core\_web\_sm} model to tokenize raw strings into
    lists of words and remove all punctuations.

    Note that words that occurs less than $10$ times in the corpus are removed.

    \subsection{Unigram PMI}

    We first perform unigram PMI analysis on the entire corpus, and then on the premise text and the hypothesis text
    individually.

    For each analysis experiment, we focus on the most associated words with a set of
    identity labels~\cite{identity_labels}.


    \begin{table*}
        \begin{threeparttable}
            \small
            \centering
            \begin{tabular}{l|c|c}
                \hline
                Identity & Premise & Hypothesis \\
                \hline
                women &
                saris,
                headscarves,
                bikinis,
                headdresses,
                coverings
                &
                burkas,
                husbands,
                saris,
                kimonos,
                bikinis
                \\\hline

                men &
                turbans,
                tuxedos,
                ladders,
                jumpsuits,
                wetsuits,
                &
                turbans,
                rickshaws,
                wives,
                cigars,
                tuxedos,
                \\\hline

                africans &
                tribe,
                hearts,
                tap,
                huts
                &
                armed,
                source,
                die,
                cloths,
                tribal
                \\\hline

                caucasian &
                lockers,
                handsome,
                explains,
                contemplates,
                straddling
                &
                slender,
                fleece,
                non,
                zip,
                festive
                \\\hline

                muslims\tnote{1} &
                channel,
                news,
                sponsored,
                celebrate,
                speech
                &
                christians,
                terrorists,
                celebrate,
                opening,
                phones
                \\\hline

                christians\tnote{1} &
                praising,
                lord,
                crazy,
                fun,
                woods,
                &
                muslims,
                gospel,
                impressed,
                pork,
                villagers
                \\\hline

                gay &
                pride,
                marriage,
                attendees,
                protester,
                participants,
                &
                pride,
                rights,
                marriage,
                experimenting,
                abraham
                \\\hline

                straight &
                razor,
                ahead,
                stony,
                sketch,
                crack
                &
                razor,
                tambourine,
                ahead,
                stared,
                lanes
                \\\hline

                israeli &
                desolate,
                nuts,
                pirates,
                cigarettes,
                u.s.
                &
                problems,
                eastern,
                cashier,
                cigarettes,
                counter
                \\\hline

                american &
                footballer,
                african,
                native,
                patriotic,
                south
                &
                idol,
                native,
                african,
                latin,
                drapes
                \\\hline
            \end{tabular}

            \begin{tablenotes}
                \item[1] Words that don't have associated words that occurs more than 10 times in premise text.
                The threshold is ignored for these words.
            \end{tablenotes}
        \end{threeparttable}

        \caption{Top associated unigrams with identity labels}
        \label{tab:unigram_pmi}
    \end{table*}
    % \begin{tabular}{@{}c@{}}
    % \end{tabular}


    \section{Results and Discussion}

    \subsection{Unigram PMI}

    Table~\ref{tab:unigram_pmi} lists top associated unigrams with some of the identity labels in the entire corpus,
    in premise text, and in hypothesis text.
    We can easily spot some alarmingly biased word association.
    For example, \texttt{muslims} is highly associated with \texttt{terrorists}, \texttt{africans} co-occur with
    \texttt{armed} and \texttt{die} frequently, while \texttt{caucasian} is often associated with \texttt{handsome}.

    There are also many implicit biases or stereotypes in the data.
    \texttt{women} is most associated with words related to fashion, while \texttt{men} with words about
    tools and work clothes.
    This implies that women spend more time on fancy clothes while men work on labor
    jobs and don't care about their looks, which is not true.

    Meanwhile, we can spot more heavily biased word associations in the hypothesis data compared to premises.
    For example, associations between \texttt{muslims} and \texttt{terrorists} and between
    \texttt{women} and \texttt{gossip} is only present in the hypotheses.

    However, there are yet some cases where the bias is more prevalent in the premises.
    For example, \texttt{israeli} is most associated with \texttt{desolate} and \texttt{pirates} in premises,
    compared to \texttt{problems}, \texttt{eastern}, \texttt{cashier} and so on in hypotheses.

    We also find it interesting that \texttt{south} is among the top 5 most associated words with \texttt{american}
    while \texttt{north} is not found even in the top 20s, as if people think \texttt{american} are from North
    America by default.

    \subsection{Qualitative Analysis}

    \subsection{Crowdsourcing Setup}

    \clearpage
    \bibliography{anthology,custom}
    \bibliographystyle{acl_natbib}

    % \appendix

    % \section{Example Appendix}
    % \label{sec:appendix}

    % This is an appendix.

\end{document}
